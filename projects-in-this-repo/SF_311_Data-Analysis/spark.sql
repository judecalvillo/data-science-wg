DROP TABLE IF EXISTS calls_strings;
CREATE TABLE calls_strings (case_id string, opened string, closed string, updated string, status string, status_notes string, responsible_agency string, category string, request_type string, request_details string, address string, supervisor_district string, neighborhood string, point string, source string, media_url string) 
USING com.databricks.spark.csv
OPTIONS (path "rows.csv", header "true", mode "DROPMALFORMED"); -- some rows have the newline character character in the middle of the quoted free form text fields, which the Spark CSV plugin isn't able to handle at this time

--SELECT count(*) FROM calls_strings;

--SELECT request_type, count(*) AS total FROM calls_strings GROUP BY request_type ORDER BY total DESC;

DROP TABLE IF EXISTS calls_txt;
CREATE TABLE calls_txt 
AS 
SELECT cast(case_id AS INT) AS case_id, unix_timestamp(opened, 'MM/dd/yyyy hh:mm:ss a') AS opened, unix_timestamp(closed, 'MM/dd/yyyy hh:mm:ss a') AS closed, unix_timestamp(updated, 'MM/dd/yyyy hh:mm:ss a') AS updated, status, status_notes, responsible_agency, category, request_type, request_details, address, supervisor_district, neighborhood, cast(regexp_extract(point,'\\((\\S+), (\\S+)\\)$', 1) AS DECIMAL(15, 12)) AS lat, cast(regexp_extract(point,'\\((\\S+), (\\S+)\\)$', 2) AS DECIMAL(15, 12)) AS lon , source, media_url FROM calls_strings;
-- Although not an elegant solution, I had to copy each part file generated by the
-- above table create into my Cloudera Quickstart VM. Otherwise, the VM would crash.
